<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Understanding Artificial Neural Networks | Uday Sharma </title> <meta name="author" content="Uday Sharma"> <meta name="description" content="A description of how artificial neural networks work at code and what is the idea behind them."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://usyntest.github.io/blog/2024/neural-networks/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Uday Sharma </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/resume.pdf">resume </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Understanding Artificial Neural Networks</h1> <p class="post-meta"> Created in December 24, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> ai,</a>   <a href="/blog/tag/neural-networks"> <i class="fa-solid fa-hashtag fa-sm"></i> neural-networks</a>   ·   <a href="/blog/category/machine-learning"> <i class="fa-solid fa-tag fa-sm"></i> machine-learning,</a>   <a href="/blog/category/math"> <i class="fa-solid fa-tag fa-sm"></i> math</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="introduction">Introduction</h2> <p>Artificial Intelligence (AI) is all the hype these days, every other day OpenAI, Google, or Meta is making headlines with a breakthrough in the field. Every other day we get a paper with a new state of the art method for doing something. When writing this Meta recently came out with its new LLM architecture Large Concept Models (LCMs).</p> <p><em>Why isn’t this blog about LCMs?</em> Understanding these breakthroughs is not an easy task and as you know “Rome wasn’t built in a day”. These concepts require you to have basic understanding of machine learning and deep learning, we cannot just jump onto stuff like LLMs, Transformers, CNNs, etc.</p> <p>Artificial Neural Networks (ANNs) help us build this foundational knowledge in deep learning to move to advanced topics. As a matter of fact, convolutional neural networks (CNNs) which are prominently used to recognize objects in images are a special type of neural networks.</p> <p>This blog will focus on how ANNs work at core and we’ll also build a neural network from scratch and a scalar auto-gradient library without using any library of any sorts to which will shed a little light on the foundations of deep learning.</p> <hr> <h2 id="what-are-neural-networks">What are neural networks?</h2> <p>Artificial Neural Networks (ANNs) are known by several names such as multilayer perceptrons, deep feedforward networks, feedforward neural networks, or just neural networks. ANNs comes under the category of Supervised Learning where a mapping from \(x \to y\) is provided to the algorithm and from there it learns complex relationships between independent and dependent variables. This mapping which we provide to the neural network is called the <strong>training data</strong>.</p> <p>The goal of neural networks is to approximate some function \(f^*\) so that we can predict the value of a given set of inputs based on the data provided during the training. Neural networks just like other learning algorithms like linear regression or SVM are just a mathematical function.</p> \[y = f_{NN}(x)\] <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/neural_net_structure-480.webp 480w,/assets/img/neural_net_structure-800.webp 800w,/assets/img/neural_net_structure-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/neural_net_structure.jpeg" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Neural Networks are structures made up of layers of neurons laid out sequentially. The function \(f_{NN}\) has a particular form: it’s a nested function. Each layer of NN can be thought of as one level of nesting in the \(f_{NN}\) function. The structure of neural networks allows the algorithm to learn complex decision boundaries which is not possible in algorithms like linear Regression, logistic regression, etc.</p> \[y = f_{NN} = f_3(f_2(f_1(x)))\] <p>Neural networks also eliminate the need for doing steps like feature engineering as the structure is not just a single function but a combination of different functions which are able to evaluate many combinations of data.</p> <p>There is a misconception that ANNs try to mimic the human brain or the way it works which is wrong, a better statement would be that ANNs are inspired by the structure of the brain. We don’t have enough information on how brains work or function to model it mathematically.</p> <hr> <h2 id="neurons">Neurons</h2> <p>A neuron is the smallest unit in the neural network, these are the building blocks of neural networks. It can be thought of as a simple mathematical function where inputs are provided and output is received. Every neuron has an activation function which is applied to the input data to get an output. Multiple neurons combine to create a layer of neural network and multiple layers then combine to create the entire neural network.</p> <p>The image shown above is an example of a neuron, as you can see a neuron body is provided with multiple inputs \((x_i)\), and every input is given a weight \((w_i)\). Along with these inputs, there is also a variable bias \((b)\), an activation function is applied to these inputs to get an output.</p> \[f(x, w, b) = x_1w_1 + x_2w_2 + \cdots + x_iw_i + b\] <hr> <h2 id="activation-functions">Activation Functions</h2> <p>Activation functions play a crucial role in neural networks by introducing non-linearity, allowing the network to learn complex patterns. Without activation functions, a neural network would behave like a simple linear model, limiting its ability to solve real-world problems.</p> <h5 id="1-sigmoid-activation-function">1. Sigmoid Activation Function</h5> <p>The <strong>sigmoid</strong> function maps any input value to a range between 0 and 1. It is commonly used in binary classification problems.</p> \[\sigma(x) = \frac{1}{1 + e^{-x}}\] <h6 id="properties">Properties:</h6> <ul> <li>Output range: (0,1)</li> <li>Differentiable, smooth curve</li> <li>Causes vanishing gradient problem for very large or very small inputs</li> </ul> <h5 id="2-relu-rectified-linear-unit">2. ReLU (Rectified Linear Unit)</h5> <p>ReLU is one of the most popular activation functions in deep learning. It allows positive values to pass through while converting negative values to zero.</p> \[ReLU(x) = \max(0, x)\] <h6 id="properties-1">Properties:</h6> <ul> <li>Output range: [0, ∞)</li> <li>Fast computation</li> <li>Helps in reducing vanishing gradient issue</li> <li>Can cause <strong>dying ReLU problem</strong> where neurons get stuck at 0</li> </ul> <h5 id="3-leaky-relu">3. Leaky ReLU</h5> <p>Leaky ReLU improves upon ReLU by allowing small negative values instead of setting them to zero, preventing neurons from dying.</p> \[LeakyReLU(x) = \max(\alpha x, x)\] <p>Where \(\alpha\) is a small positive constant (e.g., 0.01).</p> <h6 id="properties-2">Properties:</h6> <ul> <li>Output range: (-∞, ∞)</li> <li>Prevents dead neurons problem</li> </ul> <h5 id="4-tanh-hyperbolic-tangent">4. Tanh (Hyperbolic Tangent)</h5> <p>Tanh is similar to sigmoid but outputs values between -1 and 1, making it zero-centered, which can help in training.</p> \[tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}\] <h6 id="properties-3">Properties:</h6> <ul> <li>Output range: (-1,1)</li> <li>Zero-centered, better than sigmoid</li> <li>Still suffers from vanishing gradient problem</li> </ul> <h5 id="5-softmax-activation">5. Softmax Activation</h5> <p>The <strong>softmax</strong> function is used in the output layer for multi-class classification problems. It converts raw scores into probabilities.</p> \[softmax(x_i) = \frac{e^{x_i}}{\sum_{j} e^{x_j}}\] <h6 id="properties-4">Properties:</h6> <ul> <li>Converts outputs into probability distribution</li> <li>Used in classification tasks where multiple classes are involved</li> </ul> <h4 id="choosing-the-right-activation-function">Choosing the Right Activation Function</h4> <ul> <li> <strong>Hidden Layers:</strong> Use <strong>ReLU</strong> or <strong>Leaky ReLU</strong> for deep networks.</li> <li> <strong>Output Layer:</strong> <ul> <li>Use <strong>Sigmoid</strong> for binary classification.</li> <li>Use <strong>Softmax</strong> for multi-class classification.</li> <li>Use <strong>Tanh</strong> when outputs need to be zero-centered.</li> </ul> </li> </ul> <p>Activation functions are a key part of how neural networks learn. The right choice depends on the problem being solved and the depth of the network.</p> <hr> <h2 id="layers-and-architectures">Layers and Architectures</h2> <p>Now that we understand neurons, let’s see how they come together to form a neural network. A neural network is made up of <strong>layers of neurons</strong>, where each layer processes information and passes it forward.</p> <h5 id="types-of-layers">Types of Layers</h5> <h6 id="1-input-layer">1. Input Layer:</h6> <p>This is the first layer of the network that takes in the raw data. Each neuron in this layer represents a feature from the dataset.</p> <h6 id="2-hidden-layers">2. Hidden Layers:</h6> <p>These are the layers between the input and output layers. They perform most of the computations and learn complex patterns. A neural network with multiple hidden layers is called a <strong>deep neural network</strong>.</p> <h6 id="3-output-layer">3. Output Layer:</h6> <p>The last layer of the network gives the final prediction. The number of neurons here depends on the type of problem.</p> <ul> <li> <strong>For regression</strong>, there’s usually one neuron giving a continuous output.</li> <li> <strong>For classification</strong>, the number of neurons matches the number of classes.</li> </ul> <h5 id="architecture-of-a-neural-network">Architecture of a Neural Network</h5> <p>The structure of a neural network is determined by:</p> <ul> <li> <strong>Number of layers</strong>: More layers mean the network can learn more complex patterns.</li> <li> <strong>Number of neurons per layer</strong>: More neurons can capture more details but may lead to overfitting.</li> <li> <strong>Connections between layers</strong>: Most networks are <strong>fully connected</strong>, meaning each neuron in one layer connects to every neuron in the next layer.</li> </ul> <h5 id="choosing-the-right-architecture">Choosing the Right Architecture</h5> <p>Designing a neural network isn’t just about adding more layers and neurons. Too many can lead to <strong>overfitting</strong>, while too few might not capture enough patterns. The best approach is to experiment and fine-tune based on the problem.</p> <p>The architecture of a network depends on the task:</p> <ul> <li> <strong>Feedforward Neural Networks (FNNs)</strong>: Used for general-purpose tasks.</li> <li> <strong>Convolutional Neural Networks (CNNs)</strong>: Best for image-related tasks.</li> <li> <strong>Recurrent Neural Networks (RNNs)</strong>: Used for sequential data like text and time series.</li> </ul> <p>Each of these architectures has its strengths, and choosing the right one depends on the problem at hand.</p> <hr> <h2 id="forward-propagation">Forward Propagation</h2> <p>Now that we know how neural networks are structured, let’s see how they actually make predictions. The process of passing inputs through the network to get an output is called <strong>forward propagation</strong>. Step-by-Step Process for it is the following:</p> <h6 id="1-take-inputs">1. Take Inputs</h6> <p>The input layer receives the raw data and passes it to the next layer. Each neuron in this layer represents a feature from the dataset.</p> <h6 id="2-apply-weights-and-biases">2. Apply Weights and Biases</h6> <p>Each connection between neurons has a <strong>weight</strong> that determines how important an input is. Each neuron also has a <strong>bias</strong> to adjust the output.</p> <p>The neuron calculates the weighted sum of inputs plus the bias:</p> \[z = x_1w_1 + x_2w_2 + \dots + x_nw_n + b\] <h6 id="3-apply-activation-function">3. Apply Activation Function</h6> <p>The weighted sum is then passed through an <strong>activation function</strong>, which introduces non-linearity and helps the network learn complex patterns:</p> \[a = f(z)\] <h6 id="4-pass-to-the-next-layer">4. Pass to the Next Layer</h6> <p>The output of each neuron becomes the input for the next layer, repeating the same process until the output layer is reached.</p> <h6 id="5-generate-final-output">5. Generate Final Output</h6> <p>The last layer produces the final prediction. If it’s a classification task, this could be a probability score. If it’s regression, it could be a continuous value.</p> <h5 id="why-forward-propagation">Why Forward Propagation?</h5> <p>Forward propagation is the foundation of neural networks. It helps the network process data step by step, layer by layer, to make predictions. However, just passing data forward isn’t enough—neural networks also <strong>learn</strong> by adjusting their weights. That’s where <strong>backpropagation</strong> comes in, which we’ll discuss next.</p> <hr> <h2 id="loss-functions-and-backpropagation">Loss Functions and Backpropagation</h2> <p>Now that we understand forward propagation, we need to answer a crucial question: <strong>How does a neural network learn?</strong></p> <p>When we first initialize a neural network, the weights and biases are set randomly, meaning the predictions it makes will be completely off. Our goal is to <strong>adjust the weights and biases</strong> so that the network makes accurate predictions. This is done using two key concepts:</p> <ol> <li> <strong>Loss Functions</strong> – Measure how far the predictions are from the actual values.</li> <li> <strong>Backpropagation</strong> – Adjusts the weights and biases to reduce the loss and improve accuracy.</li> </ol> <p>Let’s go step by step.</p> <hr> <h2 id="loss-functions-measuring-error">Loss Functions: Measuring Error</h2> <p>A neural network learns by minimizing error, and to measure this error, we use a <strong>loss function</strong>.</p> <p>A loss function compares the network’s predicted output <strong>\(y_{\text{pred}}\)</strong> with the actual output <strong>\(y_{\text{true}}\)</strong> and gives a numerical value indicating how bad the prediction was. The goal is to make this number as <strong>small as possible</strong>.</p> <h3 id="common-loss-functions">Common Loss Functions</h3> <p>Depending on the type of problem, different loss functions are used:</p> <h6 id="1-mean-squared-error-mse--for-regression-problems">1. Mean Squared Error (MSE) – For Regression Problems</h6> <p>Used when predicting continuous values. It calculates the squared difference between the actual and predicted values:</p> \[L = \frac{1}{n} \sum (y_{\text{true}} - y_{\text{pred}})^2\] <ul> <li>Large errors contribute more due to squaring.</li> <li>Encourages smaller differences between predicted and actual values.</li> </ul> <h6 id="2-cross-entropy-loss--for-classification-problems">2. Cross-Entropy Loss – For Classification Problems</h6> <p>Used when predicting categories (e.g., cat vs. dog). If we have multiple classes, cross-entropy loss is used to measure how different the predicted probability distribution is from the actual labels:</p> \[L = -\sum y_{\text{true}} \log(y_{\text{pred}})\] <ul> <li>Punishes incorrect confident predictions heavily.</li> <li>Encourages the model to assign high probabilities to the correct class.</li> </ul> <h3 id="why-is-loss-important">Why is Loss Important?</h3> <p>The loss function tells us <strong>how good or bad</strong> our neural network is. But knowing this isn’t enough—we also need a way to improve the network. That’s where <strong>backpropagation</strong> comes in.</p> <hr> <h2 id="backpropagation-how-neural-networks-learn">Backpropagation: How Neural Networks Learn</h2> <p>Backpropagation is the heart of training a neural network. It’s the process of <strong>adjusting weights and biases</strong> to reduce the loss, making the network better over time.</p> <h5 id="the-core-idea">The Core Idea</h5> <p>Backpropagation works by moving <strong>backward</strong> through the network and tweaking the weights in the direction that reduces the loss. It uses an optimization algorithm called <strong>gradient descent</strong>, which we’ll discuss shortly.</p> <h5 id="step-by-step-explanation">Step-by-Step Explanation</h5> <h6 id="1-compute-the-loss">1. Compute the Loss</h6> <p>First, we calculate the loss using a loss function (MSE or cross-entropy).</p> <h6 id="2-calculate-gradients-derivatives">2. Calculate Gradients (Derivatives)</h6> <p>The loss depends on the weights and biases. Backpropagation computes <strong>how much each weight and bias contributed to the loss</strong> using <strong>partial derivatives</strong>.</p> <p>Mathematically, we find:</p> \[\frac{\partial L}{\partial w}\] <p>This tells us:</p> <ul> <li>If \(\frac{\partial L}{\partial w}\) is <strong>positive</strong>, decreasing \(w\) will reduce loss.</li> <li>If \(\frac{\partial L}{\partial w}\) is <strong>negative</strong>, increasing \(w\) will reduce loss.</li> </ul> <h6 id="3-update-weights-using-gradient-descent">3. Update Weights Using Gradient Descent</h6> <p>We adjust each weight using <strong>gradient descent</strong>, which updates weights in the opposite direction of the gradient:</p> \[w_{\text{new}} = w_{\text{old}} - \eta \cdot \frac{\partial L}{\partial w}\] <ul> <li>\(\eta\) is the <strong>learning rate</strong>, controlling how big the step is.</li> <li>A small learning rate makes learning slow.</li> <li>A large learning rate can cause instability.</li> </ul> <h6 id="4-repeat-until-convergence">4. Repeat Until Convergence</h6> <ul> <li>We perform this update for all weights and biases.</li> <li>The process repeats for multiple iterations (epochs) until the loss is small.</li> </ul> <h4 id="understanding-gradient-descent">Understanding Gradient Descent</h4> <p>Gradient descent is how backpropagation <strong>adjusts</strong> weights to reduce the loss. It finds the lowest point on the loss surface, like rolling a ball downhill.</p> <h5 id="types-of-gradient-descent">Types of Gradient Descent</h5> <h6 id="1-batch-gradient-descent">1. Batch Gradient Descent</h6> <ul> <li>Uses all training data to compute gradients.</li> <li>More stable but slower.</li> </ul> <h6 id="2-stochastic-gradient-descent-sgd">2. Stochastic Gradient Descent (SGD)</h6> <ul> <li>Updates weights after each training example.</li> <li>Faster but noisier updates.</li> </ul> <h6 id="3-mini-batch-gradient-descent">3. Mini-Batch Gradient Descent</h6> <ul> <li>Uses small batches of data for updates.</li> <li>Balances speed and stability.</li> </ul> <h5 id="why-does-backpropagation-work">Why Does Backpropagation Work?</h5> <p>Backpropagation is based on the <strong>chain rule of calculus</strong>. Since a neural network is just a <strong>nested function</strong>, we compute derivatives layer by layer:</p> \[\frac{\partial L}{\partial w} = \frac{\partial L}{\partial a} \times \frac{\partial a}{\partial z} \times \frac{\partial z}{\partial w}\] <p>Each layer gets its error <strong>gradually corrected</strong> by passing gradients backward, ensuring earlier layers learn the right representations.</p> <hr> <h2 id="summary-the-learning-cycle">Summary: The Learning Cycle</h2> <ol> <li> <p>Forward Propagation: Inputs pass through the network to generate predictions.</p> </li> <li> <p>Compute Loss: Compare predictions to actual values.</p> </li> <li> <p>Backpropagation: Compute gradients of loss w.r.t. weights.</p> </li> <li> <p>Gradient Descent:Adjust weights to minimize loss.</p> </li> <li> <p>Repeat: Continue until loss is small.</p> </li> </ol> <p>This is how neural networks learn—by continuously refining their weights using loss functions and backpropagation.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/cern-selection/">From Delhi to Geneva: My Journey to CERN</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/deployment-docker/">Deploying Applications Using Nginx and Docker</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/vector-databases/">Introduction to Vector Databases</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Uday Sharma. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?70d799092f862ad98c7876aa47712e20"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>