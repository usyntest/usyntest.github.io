<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://usyntest.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://usyntest.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-03-09T13:23:49+00:00</updated><id>https://usyntest.github.io/feed.xml</id><title type="html">Usyntest</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Introduction to Vector Databases</title><link href="https://usyntest.github.io/blog/2024/vector-databases/" rel="alternate" type="text/html" title="Introduction to Vector Databases"/><published>2024-12-24T16:40:16+00:00</published><updated>2024-12-24T16:40:16+00:00</updated><id>https://usyntest.github.io/blog/2024/vector-databases</id><content type="html" xml:base="https://usyntest.github.io/blog/2024/vector-databases/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Traditional databases are great at storing and retrieving structured data—user records, transactions, logs—where exact matching works well. But what if you want to search for similar things instead of exact matches?</p> <p>Think of searching for an image of a golden retriever. A traditional database would need exact metadata tags like <code class="language-plaintext highlighter-rouge">"dog"</code> or <code class="language-plaintext highlighter-rouge">"golden retriever"</code> to match it. But what if you only had the image itself? This is where vector databases come in.</p> <p>Vector databases store high-dimensional embeddings, numerical representations of unstructured data like text, images, and audio. Instead of keyword-based searches, they use Approximate Nearest Neighbor (ANN) search to find similar vectors based on distance metrics like cosine similarity or Euclidean distance.</p> <p>With the rise of AI and machine learning, applications like semantic search, recommendation systems, anomaly detection, and AI chatbots rely heavily on vector search. While some traditional databases offer vector search extensions (e.g., PostgreSQL’s pgvector), dedicated vector databases like Milvus, Qdrant, Weaviate, and Pinecone are built specifically for high-speed, large-scale similarity search.</p> <p>In this blog, I’ll break down:</p> <ol> <li>What vector databases are and how they work?</li> <li>How they differ from SQL and NoSQL databases?</li> <li>Why they are essential for modern AI-driven applications?</li> <li>The key indexing techniques behind vector search</li> <li>How to get started with vector databases?</li> </ol> <hr/> <h2 id="what-are-vector-databases">What are Vector Databases?</h2> <p>Vector databases are specialized databases designed to store and retrieve high-dimensional vector embeddings efficiently. Unlike traditional databases that rely on exact matching, vector databases perform similarity search by comparing numerical representations of data points.</p> <h5 id="key-concept-storing-and-searching-embeddings">Key Concept: Storing and Searching Embeddings</h5> <p>Machine learning models can convert unstructured data—text, images, audio—into vector embeddings, which are numerical arrays that capture the meaning and relationships within the data. These embeddings are stored in a vector database, allowing for Approximate Nearest Neighbor (ANN) search to quickly find similar items.</p> <p>For example:</p> <ul> <li>A text embedding might represent the meaning of a sentence, enabling semantic search (e.g., <code class="language-plaintext highlighter-rouge">"cheap laptop"</code> matching <code class="language-plaintext highlighter-rouge">"affordable notebook"</code>).</li> <li>An image embedding might encode visual features, enabling image similarity search (e.g., retrieving dog photos similar to a given one).</li> </ul> <p>Instead of retrieving exact matches, vector databases return results ranked by how similar they are to the query, based on distance metrics like cosine similarity, Euclidean distance, or dot product.</p> <hr/> <h2 id="how-vector-search-works">How Vector Search Works</h2> <p>Vector search operates on the principle of <strong>finding the closest vectors in high-dimensional space</strong>. Since storing and searching millions (or billions) of vectors is computationally expensive, efficient algorithms like Approximate Nearest Neighbor (ANN) search are used instead of brute-force comparisons.</p> <h5 id="distance-metrics-for-similarity-search">Distance Metrics for Similarity Search</h5> <p>To determine how “close” or “similar” two vectors are, vector databases use distance functions such as:</p> <h6 id="1-euclidean-distance-l2-norm">1. Euclidean Distance (L2 norm)</h6> <p>Measures the straight-line distance between two points in space. Best for numeric data and structured patterns.</p> \[d(x, y) = \sqrt{\sum (x_i - y_i)^2}\] <h6 id="2-cosine-similarity">2. Cosine Similarity</h6> <p>Measures the angle between two vectors, focusing on direction rather than magnitude. Often used for <strong>text embeddings</strong>.</p> \[\text{similarity} = \frac{x \cdot y}{\|x\| \|y\|}\] <h6 id="3-dot-product-similarity">3. Dot Product Similarity</h6> <p>A variation of cosine similarity, often used in recommendation systems where larger values indicate stronger similarity.</p> \[\text{similarity} = x \cdot y\] <p>Depending on the use case (text search, image retrieval, recommendation systems), different databases and models optimize for specific distance metrics.</p> <hr/> <h2 id="indexing-techniques-in-vector-databases">Indexing Techniques in Vector Databases</h2> <p>Since performing a brute-force search across millions of vectors is too slow, vector databases use indexing techniques to speed up search by organizing and clustering vectors efficiently. Here are some of the most common indexing methods:</p> <h6 id="1-hnsw-hierarchical-navigable-small-world">1. HNSW (Hierarchical Navigable Small World)</h6> <ul> <li>A graph-based approach that builds a multi-layered network of vectors.</li> <li>Uses greedy search to quickly navigate through connected nodes to find the nearest neighbors.</li> <li>Provides fast and accurate searches, widely used in Qdrant, FAISS, and Milvus.</li> </ul> <h6 id="2-ivf-inverted-file-index">2. IVF (Inverted File Index)</h6> <ul> <li>Clusters vectors into Voronoi cells and searches only within the relevant cluster.</li> <li>Instead of scanning all vectors, it narrows down search space, making it memory efficient.</li> <li>Used in FAISS and Milvus for large-scale vector retrieval.</li> </ul> <h6 id="3-pq-product-quantization">3. PQ (Product Quantization)</h6> <ul> <li>Compresses high-dimensional vectors into smaller representations using quantization techniques.</li> <li>Useful when dealing with limited memory resources, as it reduces the storage size of vectors.</li> <li>Works well in combination with IVF for scalable vector search.</li> </ul> <h5 id="choosing-the-right-indexing-method">Choosing the Right Indexing Method</h5> <table> <thead> <tr> <th>Method</th> <th>Best For</th> <th>Pros</th> <th>Cons</th> </tr> </thead> <tbody> <tr> <td>HNSW</td> <td>High-accuracy search</td> <td>Fast &amp; scalable</td> <td>High memory usage</td> </tr> <tr> <td>IVF</td> <td>Large-scale retrieval</td> <td>Memory-efficient</td> <td>Lower recall for small datasets</td> </tr> <tr> <td>PQ</td> <td>Memory optimization</td> <td>Reduces storage size</td> <td>Slight drop in accuracy</td> </tr> </tbody> </table> <blockquote> <p><strong>Note:</strong> Each indexing method balances speed, memory efficiency, and accuracy. The right choice depends on the dataset size, query speed requirements, and available resources.</p> </blockquote> <hr/> <h2 id="use-cases-of-vector-databases">Use Cases of Vector Databases</h2> <p>Vector databases power many AI-driven applications where <strong>similarity search</strong> is crucial. Instead of relying on exact keyword matches, these databases allow systems to find <strong>semantically similar</strong> results, making them essential for:</p> <h6 id="1-semantic-search">1. Semantic Search</h6> <p>Traditional keyword-based search systems struggle with synonyms and contextual meaning. Vector databases enable <strong>semantic search</strong>, where search queries are matched based on their meaning rather than exact words.</p> <p><strong>Example:</strong> Searching for <code class="language-plaintext highlighter-rouge">"affordable phone"</code> returns <code class="language-plaintext highlighter-rouge">"cheap smartphone deals"</code> even though the words are different.</p> <h6 id="2-image--video-search">2. Image &amp; Video Search</h6> <p>Vector embeddings allow <strong>reverse image search</strong>, where users can upload an image to find visually similar ones.</p> <p><strong>Example:</strong> Google Lens and Pinterest use vector embeddings to match images with similar content.</p> <h6 id="3-personalized-recommendations">3. Personalized Recommendations</h6> <p>Recommendation systems use vector embeddings to suggest <strong>similar items</strong> based on user preferences.</p> <p><strong>Example:</strong></p> <ul> <li><strong>E-commerce:</strong> <code class="language-plaintext highlighter-rouge">"Users who liked this product also liked..."</code></li> <li><strong>Streaming services:</strong> <code class="language-plaintext highlighter-rouge">"Because you watched X, you might like Y"</code></li> </ul> <h6 id="4-anomaly-detection--fraud-prevention">4. Anomaly Detection &amp; Fraud Prevention</h6> <p>Vector search helps detect anomalies by identifying outliers in high-dimensional data.</p> <p><strong>Example:</strong></p> <ul> <li><strong>Finance:</strong> Detecting fraudulent transactions by spotting unusual spending patterns.</li> <li><strong>Cybersecurity:</strong> Identifying unusual login behavior in a system.</li> </ul> <h6 id="5-ai-chatbots--rag-retrieval-augmented-generation">5. AI Chatbots &amp; RAG (Retrieval-Augmented Generation)</h6> <p>Vector databases improve AI chatbots by enabling <strong>context-aware responses</strong> through similarity search.</p> <p><strong>Example:</strong> ChatGPT-like assistants use vector search to retrieve relevant documents for better answers.</p> <h2 id="challenges--limitations">Challenges &amp; Limitations</h2> <p>While vector databases are powerful, they come with trade-offs:</p> <h6 id="1-memory-usage">1. Memory Usage</h6> <ul> <li>Graph-based indexing (HNSW) can be <strong>memory-intensive</strong>, requiring large RAM for high recall.</li> <li>PQ and IVF reduce memory usage but may sacrifice accuracy.</li> </ul> <h6 id="2-indexing-speed-vs-query-speed">2. Indexing Speed vs. Query Speed</h6> <ul> <li><strong>HNSW:</strong> Fast queries but slow indexing.</li> <li><strong>IVF:</strong> Faster indexing but slightly slower queries.</li> <li><strong>PQ:</strong> Best for reducing memory but adds latency to queries.</li> </ul> <h6 id="3-precision-vs-recall">3. Precision vs. Recall</h6> <ul> <li>Approximate nearest neighbor (ANN) methods <strong>prioritize speed over perfect accuracy</strong>.</li> <li>Fine-tuning parameters like <strong>ef_construction (HNSW) or nprobe (IVF)</strong> is required to balance speed and accuracy.</li> </ul> <hr/> <h2 id="getting-started-with-a-vector-database-qdrant-example">Getting Started with a Vector Database (Qdrant Example)</h2> <p>Let’s set up <strong>Qdrant</strong>, an open-source vector database, and run a <strong>basic similarity search</strong>.</p> <h6 id="1-install-qdrant-server">1. Install Qdrant Server</h6> <p>Run Qdrant locally using Docker:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker run <span class="nt">-p</span> 6333:6333 qdrant/qdrant
</code></pre></div></div> <h6 id="2-install-qdrant-client">2. Install Qdrant Client</h6> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>qdrant-client numpy
</code></pre></div></div> <h6 id="3-create-a-collection--insert-vectors">3. Create a Collection &amp; Insert Vectors</h6> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">qdrant_client</span> <span class="kn">import</span> <span class="n">QdrantClient</span>
<span class="kn">from</span> <span class="n">qdrant_client.models</span> <span class="kn">import</span> <span class="n">PointStruct</span>

<span class="c1"># Connect to Qdrant server
</span><span class="n">client</span> <span class="o">=</span> <span class="nc">QdrantClient</span><span class="p">(</span><span class="sh">"</span><span class="s">localhost</span><span class="sh">"</span><span class="p">,</span> <span class="n">port</span><span class="o">=</span><span class="mi">6333</span><span class="p">)</span>

<span class="c1"># Create a collection
</span><span class="n">client</span><span class="p">.</span><span class="nf">recreate_collection</span><span class="p">(</span><span class="n">collection_name</span><span class="o">=</span><span class="sh">"</span><span class="s">my_vectors</span><span class="sh">"</span><span class="p">,</span> <span class="n">vector_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">distance</span><span class="o">=</span><span class="sh">"</span><span class="s">Cosine</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Insert some vectors
</span><span class="n">vectors</span> <span class="o">=</span> <span class="p">[</span>
    <span class="nc">PointStruct</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">vector</span><span class="o">=</span><span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]),</span>
    <span class="nc">PointStruct</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">vector</span><span class="o">=</span><span class="p">[</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">]),</span>
    <span class="nc">PointStruct</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">vector</span><span class="o">=</span><span class="p">[</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">]),</span>
<span class="p">]</span>
<span class="n">client</span><span class="p">.</span><span class="nf">upsert</span><span class="p">(</span><span class="n">collection_name</span><span class="o">=</span><span class="sh">"</span><span class="s">my_vectors</span><span class="sh">"</span><span class="p">,</span> <span class="n">points</span><span class="o">=</span><span class="n">vectors</span><span class="p">)</span>
</code></pre></div></div> <h6 id="4-perform-a-similarity-search">4. Perform a Similarity Search</h6> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">query_vector</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.15</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.35</span><span class="p">]</span>

<span class="n">results</span> <span class="o">=</span> <span class="n">client</span><span class="p">.</span><span class="nf">search</span><span class="p">(</span>
    <span class="n">collection_name</span><span class="o">=</span><span class="sh">"</span><span class="s">my_vectors</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">query_vector</span><span class="o">=</span><span class="n">query_vector</span><span class="p">,</span>
    <span class="n">limit</span><span class="o">=</span><span class="mi">2</span>  <span class="c1"># Return top 2 similar vectors
</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>
</code></pre></div></div> <p>This will return the <strong>most similar vectors</strong> based on cosine similarity! 🎯</p> <hr/> <h2 id="conclusion">Conclusion</h2> <p>Vector databases are transforming AI-driven applications by enabling fast, scalable similarity search for text, images, audio, and more. As models get larger and embeddings become a core part of AI workflows, vector search will be a crucial component of modern data infrastructure.</p> <h5 id="future-trends-">Future Trends 🚀</h5> <p>✅ Hybrid Search – Combining vector and keyword search for better retrieval.<br/> ✅ Efficient Indexing – Reducing memory usage with better compression.<br/> ✅ Cloud-native Vector Databases – Fully managed solutions like Pinecone.<br/> ✅ RAG-powered AI Agents – Better AI models using retrieval-augmented generation.</p> <p>With the rise of LLMs and AI-powered search, vector databases will only become more important in the future. If you haven’t explored them yet, now’s the perfect time to start! 🚀</p>]]></content><author><name></name></author><category term="database"/><category term="search,"/><category term="vector-database,"/><category term="ai"/><summary type="html"><![CDATA[Modern applications require more than just exact search—they need similarity search, and vector databases enable this at scale.]]></summary></entry></feed>