<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://usyntest.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://usyntest.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-06-17T21:18:11+00:00</updated><id>https://usyntest.github.io/feed.xml</id><title type="html">Uday Sharma</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">From Delhi to Geneva: My Journey to CERN</title><link href="https://usyntest.github.io/blog/2025/cern-selection/" rel="alternate" type="text/html" title="From Delhi to Geneva: My Journey to CERN"/><published>2025-06-03T00:00:00+00:00</published><updated>2025-06-03T00:00:00+00:00</updated><id>https://usyntest.github.io/blog/2025/cern-selection</id><content type="html" xml:base="https://usyntest.github.io/blog/2025/cern-selection/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>CERN or European Organization for Nuclear Research every year invites students from various backgrounds to join the CERN Summer Student Program in Geneva, Switzerland. The program is open to everyone around the world resulting in around 10k applicants in total hence making it very competitive in nature.</p> <p>People from Member &amp; Associate Member states are given priority; these are countries which provide funding to CERN and in return students from these countries are prioritized to fill certain number of spots in the program. These spots once filled no matter how good you are you won’t be selected.</p> <p>India is an associate member state and as you can imagine for those few spots you have to put up a tough fight. In 2024, India had 5 spots and for that CERN received 5,273 applications making it an <strong><a href="https://cds.cern.ch/record/2932097/files/CERN-HR-STAFF-STAT-2024-RESTR.pdf">acceptance rate of 0.1%</a></strong>.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cern-acceptance-email-480.webp 480w,/assets/img/cern-acceptance-email-800.webp 800w,/assets/img/cern-acceptance-email-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/cern-acceptance-email.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="background">Background</h2> <p>I was interested in computers from a young age. I was introduced to programming by a friend of mine named <a href="https://sujal.dev/">Sujal</a> (shoutout to him) in 9th grade and I loved it. As you can imagine when you love something out of passion you start experimenting and figuring things out, so that’s what I did. I started learning to program using online tutorials and CS50 (thanks David J. Malan), which helped set the base for my love for building stuff. I wasn’t doing this to get an internship or a job—which can make you very tunnel visioned—I was just doing all this out of curiosity.</p> <p>After school I entered University of Delhi enthusiastically, thinking now I only have to study what I love. I joined Sri Guru Gobind Singh College of Commerce, which is affiliated to the University of Delhi.</p> <p>I didn’t really have access to technical mentors, role models, or a support system in college to guide me. There weren’t any tech societies, alumni to look up to, or much of a coding culture in general.</p> <p>So when I came to college, I was pretty disappointed. I had expected to find a group of like-minded people, but that wasn’t the case. I realized early on that I was on my own and would have to figure things out without much help.</p> <p>I engaged in research activities with one of my professors, Dr. Vandana Kalra, who was working on detecting COVID-19 in lung X-rays using CNNs. This became my first research internship, and after that I went on to work under Dr. Akshay Agarwal at IISER Bhopal in Biometrics. These two research internships helped me lay my foundations in Machine Learning and discover an interest that would later help me find a role that changed the course of my career.</p> <h2 id="simppl-the-inflection-point">SimPPL: The Inflection Point</h2> <p>Working as a Research Engineer at SimPPL was the holy grail in getting into the CERN Summer Student Program. I remember a good friend of mine, Priyanshi (shoutout to her), applying to a position at SimPPL but not going through with it due to other commitments. She showed me the assignment they had sent her. The assignment said that if someone could get data from Moj (a social media platform), they’d have a 90% chance of getting hired.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/swapneel-simppl-interview-ss-1-480.webp 480w,/assets/img/swapneel-simppl-interview-ss-1-800.webp 800w,/assets/img/swapneel-simppl-interview-ss-1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/swapneel-simppl-interview-ss-1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/swapneel-simppl-interview-ss-2-480.webp 480w,/assets/img/swapneel-simppl-interview-ss-2-800.webp 800w,/assets/img/swapneel-simppl-interview-ss-2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/swapneel-simppl-interview-ss-2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Messages I sent to Swapneel after I completed the assignemnt and got the interview </div> <p>As you can imagine, I got onto it—and in 3 days, I reverse-engineered Moj’s API, collected the data, and built a dashboard. After that, I gave the interview for the position of Research Engineer and got selected. This internship taught me more than I could’ve imagined, and I will forever think of it as the primary stepping stone in my career.</p> <p>The internship brought immense mentorship from two of the best people I’ve met—Dhara and Swapneel (shoutout to them)—who taught me everything from how to present ideas to solving really hard technical problems.</p> <p>After joining SimPPL, I started working as a solo developer on a project called Arbiter that had been inactive for a while. I dedicated the next 10 months of my life to developing or resurrecting Arbiter from scratch.</p> <p>I developed data pipelines from multiple social media platforms, optimized these pipelines, created an efficient backend, and integrated it with a frontend. I also built a semantic search infrastructure to search over billions of data points for more accurate social media analysis.</p> <p>In just four months, I developed the initial MVP and presented a live demo of the platform to Nest fact-checkers and civil society organizations in Mongolia, during capacity-building workshops led by SimPPL in partnership with DW Germany. Soon, instead of being the sole developer, we had a team for the project, and my role evolved into that of Technical Lead—facilitating the integration of features like an AI assistant, in-house topic modeling scatter plots, caching mechanisms for faster results, better storage architecture for large-scale data, and designing efficient database schemas.</p> <p>Apart from the real-world technical challenges, I also learned soft skills like how to design features without over-engineering simple problems. I also learned how to present to any audience—whether to seasoned engineers or to people who are not technically literate.</p> <p>I am very grateful for these soft skills I picked up at SimPPL, especially since I had no one before who told me how to ask good questions, design solutions, research topics, and present them—and the best one of all: “what are cc and bcc in an email?” (Thank you, Dhara!).</p> <h2 id="applying-to-cern-the-vulnerable-truth">Applying to CERN: The Vulnerable Truth</h2> <p>My introduction to CERN was through the movie Angels &amp; Demons, and I was fascinated by it—but never thought I’d end up working there. In a one-to-one session with Swapneel, I got introduced to the program since he had also done it during his undergrad. That’s when he told me, “We should help you prepare an application for it.” It was then that I knew I had struck gold in terms of the people around me.</p> <p>Initially, I believed it was literally impossible to get an international internship without coming from one of the IITs—and especially not being formally trained in Physics. And if I’m being honest, I wasn’t going to apply at all. It was only after Swapneel convinced me that I finally did. I made my application on the last day of the deadline.</p> <p>On March 7th, I received an email for an interview with a potential supervisor, which made me go crazy with anxiety, since interviews are very rare in the selection process for the Summer Student Program. Over the weekend, I studied everything I could about the project the interview was for—made system design diagrams, read previous years’ student reports for the project, and probably went over my 3-minute introduction at least 100 times, which took me around 12 hours (not kidding).</p> <p>I got cold feet the night before the interview and was very nervous as this was a very big interview for me. After that, he and I had a long talk, which gave me some clarity. On May 10th, I gave the interview for the position and was very happy with what I said—although I wished I could’ve said a lot more, but time didn’t allow.</p> <h2 id="acceptance-and-what-it-means">Acceptance and What It Means</h2> <p>On May 14th—the day of Holi—I remember vividly, I was sitting in the hall watching some random show when a notification popped up from CERN that said “Congratulations.” It was a moment I’ll remember for the rest of my life and will always cherish.</p> <p>I got selected out of 10,000 applicants to come to CERN and work on the things I love, which was simply astonishing. When I entered college, I never thought I’d get to attend such a prestigious student program. Working over the summer in Switzerland sounded almost too good to be true.</p> <p>Over this summer, I’ll be working at CERN in collaboration with the World Health Organization on the NeutrinoReview project, which is a Systematic Review AI Tool aimed at automating literature reviews, under the supervision of Elias Sandner. I hope to gain a lot of exposure working on this project while collaborating with such a diverse team, where people come from all around the world.</p> <p>While working at CERN, I want to learn how things are done at an organization of such scale—how projects like the LHC are brought to life, and how to carry out research that might change the discourse of humanity’s future. I’m looking forward to collaborating with some of the brightest minds of the 21st century.</p> <h2 id="how-simppl-helped-me-get-here">How SimPPL Helped me Get here</h2> <p>SimPPL taught me to think outside the box—not just about the technical aspects of development, but also from a product design point of view—which gave me a better perspective. Along with this, they gave me the freedom to test out ideas and experiment with different approaches, which helped me become more dynamic in nature.</p> <p>Working as a solo developer gave me the confidence to build and ship products and take on the challenges that come with it, like handling bugs in production, testing features, and debugging under pressure. Among other things, I also learned how to work in research–industry collaborations, which helped me build an understanding of Research &amp; Development.</p> <p>Learning to write was something I was never good at, but this habit was somewhat forced into me—and I’ll be grateful for it throughout my career. I learned how to tell my story better in applications, which ultimately led to my selection in the CERN Summer Student Program.</p> <h2 id="advice-for-future-applicants">Advice for Future Applicants</h2> <p>The deadline for the application is in late January, but try to start thinking about your application at least a month before, since the more thought you put into it, the better. At the time, the application only consisted of 4 documents: the application form, resume, motivation letter, and reference letters.</p> <p>Your resume is the most important part of your application, I believe, since selection is mostly based on whether or not your resume contains the keywords a supervisor searches for while filtering candidates. Think of your resume as the only advocate for your skills—since interviews only happen in rare cases, it’s often the only document that can vouch for you.</p> <p>Make more than one iteration of your motivation letter and ask someone else to proofread it. Also, informing the person you’re requesting a reference letter from at least a month in advance is the best way—since it gives them time to write you a thoughtful recommendation.</p> <h2 id="final-thoughts">Final Thoughts</h2> <p>Competition for opportunities is increasing day by day, which makes us tunnel-visioned—leading to negative thoughts and a lack of concern for mental health. Try to take care of yourself; everything works out in the end, but mental or physical health takes a lot of time and effort to restore.</p> <p>Everybody works at their own pace. Try not to reverse-engineer someone else’s career. Instead of chasing binary goals, focus on enjoying the learning process. Just remember—it took Virat Kohli 18 years to say <em>“Ee sala cup namde.”</em></p>]]></content><author><name></name></author><category term="career"/><category term="cern,"/><category term="experience"/><summary type="html"><![CDATA[My journey of how I got selected as a Summer Student at CERN for the batch of 2025 out of 10k people from around the world.]]></summary></entry><entry><title type="html">Deploying Applications Using Nginx and Docker</title><link href="https://usyntest.github.io/blog/2025/deployment-docker/" rel="alternate" type="text/html" title="Deploying Applications Using Nginx and Docker"/><published>2025-03-09T00:00:00+00:00</published><updated>2025-03-09T00:00:00+00:00</updated><id>https://usyntest.github.io/blog/2025/deployment-docker</id><content type="html" xml:base="https://usyntest.github.io/blog/2025/deployment-docker/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Deploying web applications can be overwhelming, especially when handling multiple services like a frontend, backend, and database. This guide will walk you through deploying a <strong>Next.js frontend</strong>, <strong>FastAPI backend</strong>, and <strong>PostgreSQL database</strong> using <strong>Docker</strong> and <strong>Nginx as a reverse proxy</strong>.</p> <p>By the end of this tutorial, you’ll understand how to:</p> <ul> <li>Use <strong>Docker Compose</strong> to manage multi-container applications.</li> <li>Configure <strong>Nginx</strong> as a reverse proxy for your frontend and backend.</li> <li>Secure your application with <strong>SSL certificates</strong>.</li> </ul> <hr/> <h2 id="understanding-the-architecture">Understanding the Architecture</h2> <p>We’ll deploy the following services:</p> <ul> <li><strong>Frontend</strong>: A Next.js application running on port <code class="language-plaintext highlighter-rouge">3000</code>.</li> <li><strong>Backend</strong>: A FastAPI service running on port <code class="language-plaintext highlighter-rouge">8000</code>.</li> <li><strong>Database</strong>: A PostgreSQL instance for data storage.</li> <li><strong>Nginx</strong>: A reverse proxy to handle traffic and SSL termination.</li> </ul> <p>Docker Compose will orchestrate these services and ensure they run together in an isolated network.</p> <hr/> <h2 id="step-1-setting-up-docker-compose">Step 1: Setting Up Docker Compose</h2> <p>Our <strong>docker-compose.yml</strong> file defines how all the services run together:</p> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">version</span><span class="pi">:</span> <span class="s1">'</span><span class="s">3.8'</span>

<span class="na">services</span><span class="pi">:</span>
  <span class="na">app</span><span class="pi">:</span>
    <span class="na">image</span><span class="pi">:</span> <span class="s">&lt;link_to_image&gt;</span>
    <span class="na">container_name</span><span class="pi">:</span> <span class="s">fastapi_app</span>
    <span class="na">restart</span><span class="pi">:</span> <span class="s">always</span>
    <span class="na">ports</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s2">"</span><span class="s">8000:8000"</span>
    <span class="na">depends_on</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">db</span>
    <span class="na">environment</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">DATABASE_URL=postgresql://${DATABASE_USERNAME}:${DATABASE_PASSWORD}@${DATABASE_HOST}:${DATABASE_PORT}/${DATABASE_NAME}</span>
    <span class="na">networks</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">app_network</span>

  <span class="na">db</span><span class="pi">:</span>
    <span class="na">image</span><span class="pi">:</span> <span class="s">postgres:latest</span>
    <span class="na">container_name</span><span class="pi">:</span> <span class="s">postgres_db</span>
    <span class="na">restart</span><span class="pi">:</span> <span class="s">always</span>
    <span class="na">environment</span><span class="pi">:</span>
      <span class="na">POSTGRES_USER</span><span class="pi">:</span> <span class="s">${DATABASE_USERNAME}</span>
      <span class="na">POSTGRES_PASSWORD</span><span class="pi">:</span> <span class="s">${DATABASE_PASSWORD}</span>
      <span class="na">POSTGRES_DB</span><span class="pi">:</span> <span class="s">${DATABASE_NAME}</span>
    <span class="na">volumes</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">postgres_data:/var/lib/postgresql/data</span>
    <span class="na">networks</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">app_network</span>

  <span class="na">nginx</span><span class="pi">:</span>
    <span class="na">image</span><span class="pi">:</span> <span class="s">nginx:latest</span>
    <span class="na">container_name</span><span class="pi">:</span> <span class="s">nginx_proxy</span>
    <span class="na">restart</span><span class="pi">:</span> <span class="s">always</span>
    <span class="na">ports</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s2">"</span><span class="s">80:80"</span>
      <span class="pi">-</span> <span class="s2">"</span><span class="s">443:443"</span>
    <span class="na">volumes</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">./nginx.conf:/etc/nginx/nginx.conf:ro</span>
      <span class="pi">-</span> <span class="s">/etc/letsencrypt/&lt;path&gt;/fullchain.pem:/etc/letsencrypt/live/&lt;path&gt;/fullchain.pem</span>
      <span class="pi">-</span> <span class="s">/etc/letsencrypt/&lt;path&gt;/privkey.pem:/etc/letsencrypt/live/&lt;path&gt;/privkey.pem</span>
    <span class="na">depends_on</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">app</span>
    <span class="na">networks</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">app_network</span>

  <span class="na">frontend</span><span class="pi">:</span>
    <span class="na">image</span><span class="pi">:</span> <span class="s">&lt;link_to_image&gt;</span>
    <span class="na">container_name</span><span class="pi">:</span> <span class="s">nextjs_frontend</span>
    <span class="na">ports</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s2">"</span><span class="s">3000:3000"</span>
    <span class="na">environment</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">NEXT_PUBLIC_API_URL=&lt;api_url&gt;</span>
    <span class="na">depends_on</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">app</span>
    <span class="na">networks</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">app_network</span>

<span class="na">networks</span><span class="pi">:</span>
  <span class="na">app_network</span><span class="pi">:</span>

<span class="na">volumes</span><span class="pi">:</span>
  <span class="na">postgres_data</span><span class="pi">:</span>
</code></pre></div></div> <hr/> <h2 id="step-2-configuring-nginx-as-a-reverse-proxy">Step 2: Configuring Nginx as a Reverse Proxy</h2> <p>The Nginx configuration file (<strong>nginx.conf</strong>) routes requests to the correct service:</p> <div class="language-nginx highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">http</span> <span class="p">{</span>
    <span class="kn">server</span> <span class="p">{</span>
        <span class="kn">listen</span> <span class="mi">80</span><span class="p">;</span>
        <span class="kn">server_name</span> <span class="s">&lt;url&gt;</span><span class="p">;</span>
        <span class="kn">return</span> <span class="mi">301</span> <span class="s">https://</span><span class="nv">$host$request_uri</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="kn">server</span> <span class="p">{</span>
        <span class="kn">listen</span> <span class="mi">443</span> <span class="s">ssl</span><span class="p">;</span>
        <span class="kn">server_name</span> <span class="s">&lt;url&gt;</span><span class="p">;</span>
        
        <span class="kn">ssl_certificate</span> <span class="n">/etc/letsencrypt/&lt;path&gt;/fullchain.pem</span><span class="p">;</span>
        <span class="kn">ssl_certificate_key</span> <span class="n">/etc/letsencrypt/&lt;path&gt;/privkey.pem</span><span class="p">;</span>

        <span class="kn">location</span> <span class="n">/</span> <span class="p">{</span>
            <span class="kn">proxy_pass</span> <span class="s">http://fastapi_app:8000</span><span class="p">;</span>
            <span class="kn">proxy_set_header</span> <span class="s">Host</span> <span class="nv">$host</span><span class="p">;</span>
            <span class="kn">proxy_set_header</span> <span class="s">X-Real-IP</span> <span class="nv">$remote_addr</span><span class="p">;</span>
            <span class="kn">proxy_set_header</span> <span class="s">X-Forwarded-For</span> <span class="nv">$proxy_add_x_forwarded_for</span><span class="p">;</span>
            <span class="kn">proxy_set_header</span> <span class="s">X-Forwarded-Proto</span> <span class="nv">$scheme</span><span class="p">;</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div> <hr/> <h2 id="step-3-running-the-application">Step 3: Running the Application</h2> <h3 id="build-and-run-the-containers">Build and Run the Containers</h3> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker-compose up <span class="nt">-d</span>
</code></pre></div></div> <p>This will start all services in the background.</p> <h3 id="verify-everything-is-running">Verify Everything is Running</h3> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker ps
</code></pre></div></div> <p>You should see containers for <code class="language-plaintext highlighter-rouge">nginx_proxy</code>, <code class="language-plaintext highlighter-rouge">fastapi_app</code>, <code class="language-plaintext highlighter-rouge">nextjs_frontend</code>, and <code class="language-plaintext highlighter-rouge">postgres_db</code>.</p> <h3 id="restarting-a-service">Restarting a Service</h3> <p>If you make changes, restart a specific service:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker-compose restart nginx
</code></pre></div></div> <hr/> <h2 id="conclusion">Conclusion</h2> <p>You’ve successfully deployed a <strong>Next.js + FastAPI application with PostgreSQL</strong>, managed with <strong>Docker and Nginx</strong>. This setup ensures: ✅ Scalable and isolated services. ✅ Secure deployment with HTTPS. ✅ Easy container management using Docker Compose.</p> <p>Now, you can focus on developing your application while Docker and Nginx handle the deployment. 🚀</p>]]></content><author><name></name></author><category term="devops,"/><category term="web-development"/><category term="deployment,"/><category term="docker"/><summary type="html"><![CDATA[A beginner-friendly guide to deploying a Next.js frontend, FastAPI backend, and PostgreSQL database using Docker and Nginx.]]></summary></entry><entry><title type="html">Understanding Artificial Neural Networks</title><link href="https://usyntest.github.io/blog/2024/neural-networks/" rel="alternate" type="text/html" title="Understanding Artificial Neural Networks"/><published>2024-12-24T16:40:16+00:00</published><updated>2024-12-24T16:40:16+00:00</updated><id>https://usyntest.github.io/blog/2024/neural-networks</id><content type="html" xml:base="https://usyntest.github.io/blog/2024/neural-networks/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Artificial Intelligence (AI) is all the hype these days, every other day OpenAI, Google, or Meta is making headlines with a breakthrough in the field. Every other day we get a paper with a new state of the art method for doing something. When writing this Meta recently came out with its new LLM architecture Large Concept Models (LCMs).</p> <p><em>Why isn’t this blog about LCMs?</em> Understanding these breakthroughs is not an easy task and as you know “Rome wasn’t built in a day”. These concepts require you to have basic understanding of machine learning and deep learning, we cannot just jump onto stuff like LLMs, Transformers, CNNs, etc.</p> <p>Artificial Neural Networks (ANNs) help us build this foundational knowledge in deep learning to move to advanced topics. As a matter of fact, convolutional neural networks (CNNs) which are prominently used to recognize objects in images are a special type of neural networks.</p> <p>This blog will focus on how ANNs work at core and we’ll also build a neural network from scratch and a scalar auto-gradient library without using any library of any sorts to which will shed a little light on the foundations of deep learning.</p> <hr/> <h2 id="what-are-neural-networks">What are neural networks?</h2> <p>Artificial Neural Networks (ANNs) are known by several names such as multilayer perceptrons, deep feedforward networks, feedforward neural networks, or just neural networks. ANNs comes under the category of Supervised Learning where a mapping from \(x \to y\) is provided to the algorithm and from there it learns complex relationships between independent and dependent variables. This mapping which we provide to the neural network is called the <strong>training data</strong>.</p> <p>The goal of neural networks is to approximate some function \(f^*\) so that we can predict the value of a given set of inputs based on the data provided during the training. Neural networks just like other learning algorithms like linear regression or SVM are just a mathematical function.</p> \[y = f_{NN}(x)\] <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/neural_net_structure-480.webp 480w,/assets/img/neural_net_structure-800.webp 800w,/assets/img/neural_net_structure-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/neural_net_structure.jpeg" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Neural Networks are structures made up of layers of neurons laid out sequentially. The function \(f_{NN}\) has a particular form: it’s a nested function. Each layer of NN can be thought of as one level of nesting in the \(f_{NN}\) function. The structure of neural networks allows the algorithm to learn complex decision boundaries which is not possible in algorithms like linear Regression, logistic regression, etc.</p> \[y = f_{NN} = f_3(f_2(f_1(x)))\] <p>Neural networks also eliminate the need for doing steps like feature engineering as the structure is not just a single function but a combination of different functions which are able to evaluate many combinations of data.</p> <p>There is a misconception that ANNs try to mimic the human brain or the way it works which is wrong, a better statement would be that ANNs are inspired by the structure of the brain. We don’t have enough information on how brains work or function to model it mathematically.</p> <hr/> <h2 id="neurons">Neurons</h2> <p>A neuron is the smallest unit in the neural network, these are the building blocks of neural networks. It can be thought of as a simple mathematical function where inputs are provided and output is received. Every neuron has an activation function which is applied to the input data to get an output. Multiple neurons combine to create a layer of neural network and multiple layers then combine to create the entire neural network.</p> <p>The image shown above is an example of a neuron, as you can see a neuron body is provided with multiple inputs \((x_i)\), and every input is given a weight \((w_i)\). Along with these inputs, there is also a variable bias \((b)\), an activation function is applied to these inputs to get an output.</p> \[f(x, w, b) = x_1w_1 + x_2w_2 + \cdots + x_iw_i + b\] <hr/> <h2 id="activation-functions">Activation Functions</h2> <p>Activation functions play a crucial role in neural networks by introducing non-linearity, allowing the network to learn complex patterns. Without activation functions, a neural network would behave like a simple linear model, limiting its ability to solve real-world problems.</p> <h5 id="1-sigmoid-activation-function">1. Sigmoid Activation Function</h5> <p>The <strong>sigmoid</strong> function maps any input value to a range between 0 and 1. It is commonly used in binary classification problems.</p> \[\sigma(x) = \frac{1}{1 + e^{-x}}\] <h6 id="properties">Properties:</h6> <ul> <li>Output range: (0,1)</li> <li>Differentiable, smooth curve</li> <li>Causes vanishing gradient problem for very large or very small inputs</li> </ul> <h5 id="2-relu-rectified-linear-unit">2. ReLU (Rectified Linear Unit)</h5> <p>ReLU is one of the most popular activation functions in deep learning. It allows positive values to pass through while converting negative values to zero.</p> \[ReLU(x) = \max(0, x)\] <h6 id="properties-1">Properties:</h6> <ul> <li>Output range: [0, ∞)</li> <li>Fast computation</li> <li>Helps in reducing vanishing gradient issue</li> <li>Can cause <strong>dying ReLU problem</strong> where neurons get stuck at 0</li> </ul> <h5 id="3-leaky-relu">3. Leaky ReLU</h5> <p>Leaky ReLU improves upon ReLU by allowing small negative values instead of setting them to zero, preventing neurons from dying.</p> \[LeakyReLU(x) = \max(\alpha x, x)\] <p>Where \(\alpha\) is a small positive constant (e.g., 0.01).</p> <h6 id="properties-2">Properties:</h6> <ul> <li>Output range: (-∞, ∞)</li> <li>Prevents dead neurons problem</li> </ul> <h5 id="4-tanh-hyperbolic-tangent">4. Tanh (Hyperbolic Tangent)</h5> <p>Tanh is similar to sigmoid but outputs values between -1 and 1, making it zero-centered, which can help in training.</p> \[tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}\] <h6 id="properties-3">Properties:</h6> <ul> <li>Output range: (-1,1)</li> <li>Zero-centered, better than sigmoid</li> <li>Still suffers from vanishing gradient problem</li> </ul> <h5 id="5-softmax-activation">5. Softmax Activation</h5> <p>The <strong>softmax</strong> function is used in the output layer for multi-class classification problems. It converts raw scores into probabilities.</p> \[softmax(x_i) = \frac{e^{x_i}}{\sum_{j} e^{x_j}}\] <h6 id="properties-4">Properties:</h6> <ul> <li>Converts outputs into probability distribution</li> <li>Used in classification tasks where multiple classes are involved</li> </ul> <h4 id="choosing-the-right-activation-function">Choosing the Right Activation Function</h4> <ul> <li><strong>Hidden Layers:</strong> Use <strong>ReLU</strong> or <strong>Leaky ReLU</strong> for deep networks.</li> <li><strong>Output Layer:</strong> <ul> <li>Use <strong>Sigmoid</strong> for binary classification.</li> <li>Use <strong>Softmax</strong> for multi-class classification.</li> <li>Use <strong>Tanh</strong> when outputs need to be zero-centered.</li> </ul> </li> </ul> <p>Activation functions are a key part of how neural networks learn. The right choice depends on the problem being solved and the depth of the network.</p> <hr/> <h2 id="layers-and-architectures">Layers and Architectures</h2> <p>Now that we understand neurons, let’s see how they come together to form a neural network. A neural network is made up of <strong>layers of neurons</strong>, where each layer processes information and passes it forward.</p> <h5 id="types-of-layers">Types of Layers</h5> <h6 id="1-input-layer">1. Input Layer:</h6> <p>This is the first layer of the network that takes in the raw data. Each neuron in this layer represents a feature from the dataset.</p> <h6 id="2-hidden-layers">2. Hidden Layers:</h6> <p>These are the layers between the input and output layers. They perform most of the computations and learn complex patterns. A neural network with multiple hidden layers is called a <strong>deep neural network</strong>.</p> <h6 id="3-output-layer">3. Output Layer:</h6> <p>The last layer of the network gives the final prediction. The number of neurons here depends on the type of problem.</p> <ul> <li><strong>For regression</strong>, there’s usually one neuron giving a continuous output.</li> <li><strong>For classification</strong>, the number of neurons matches the number of classes.</li> </ul> <h5 id="architecture-of-a-neural-network">Architecture of a Neural Network</h5> <p>The structure of a neural network is determined by:</p> <ul> <li><strong>Number of layers</strong>: More layers mean the network can learn more complex patterns.</li> <li><strong>Number of neurons per layer</strong>: More neurons can capture more details but may lead to overfitting.</li> <li><strong>Connections between layers</strong>: Most networks are <strong>fully connected</strong>, meaning each neuron in one layer connects to every neuron in the next layer.</li> </ul> <h5 id="choosing-the-right-architecture">Choosing the Right Architecture</h5> <p>Designing a neural network isn’t just about adding more layers and neurons. Too many can lead to <strong>overfitting</strong>, while too few might not capture enough patterns. The best approach is to experiment and fine-tune based on the problem.</p> <p>The architecture of a network depends on the task:</p> <ul> <li><strong>Feedforward Neural Networks (FNNs)</strong>: Used for general-purpose tasks.</li> <li><strong>Convolutional Neural Networks (CNNs)</strong>: Best for image-related tasks.</li> <li><strong>Recurrent Neural Networks (RNNs)</strong>: Used for sequential data like text and time series.</li> </ul> <p>Each of these architectures has its strengths, and choosing the right one depends on the problem at hand.</p> <hr/> <h2 id="forward-propagation">Forward Propagation</h2> <p>Now that we know how neural networks are structured, let’s see how they actually make predictions. The process of passing inputs through the network to get an output is called <strong>forward propagation</strong>. Step-by-Step Process for it is the following:</p> <h6 id="1-take-inputs">1. Take Inputs</h6> <p>The input layer receives the raw data and passes it to the next layer. Each neuron in this layer represents a feature from the dataset.</p> <h6 id="2-apply-weights-and-biases">2. Apply Weights and Biases</h6> <p>Each connection between neurons has a <strong>weight</strong> that determines how important an input is. Each neuron also has a <strong>bias</strong> to adjust the output.</p> <p>The neuron calculates the weighted sum of inputs plus the bias:</p> \[z = x_1w_1 + x_2w_2 + \dots + x_nw_n + b\] <h6 id="3-apply-activation-function">3. Apply Activation Function</h6> <p>The weighted sum is then passed through an <strong>activation function</strong>, which introduces non-linearity and helps the network learn complex patterns:</p> \[a = f(z)\] <h6 id="4-pass-to-the-next-layer">4. Pass to the Next Layer</h6> <p>The output of each neuron becomes the input for the next layer, repeating the same process until the output layer is reached.</p> <h6 id="5-generate-final-output">5. Generate Final Output</h6> <p>The last layer produces the final prediction. If it’s a classification task, this could be a probability score. If it’s regression, it could be a continuous value.</p> <h5 id="why-forward-propagation">Why Forward Propagation?</h5> <p>Forward propagation is the foundation of neural networks. It helps the network process data step by step, layer by layer, to make predictions. However, just passing data forward isn’t enough—neural networks also <strong>learn</strong> by adjusting their weights. That’s where <strong>backpropagation</strong> comes in, which we’ll discuss next.</p> <hr/> <h2 id="loss-functions-and-backpropagation">Loss Functions and Backpropagation</h2> <p>Now that we understand forward propagation, we need to answer a crucial question: <strong>How does a neural network learn?</strong></p> <p>When we first initialize a neural network, the weights and biases are set randomly, meaning the predictions it makes will be completely off. Our goal is to <strong>adjust the weights and biases</strong> so that the network makes accurate predictions. This is done using two key concepts:</p> <ol> <li><strong>Loss Functions</strong> – Measure how far the predictions are from the actual values.</li> <li><strong>Backpropagation</strong> – Adjusts the weights and biases to reduce the loss and improve accuracy.</li> </ol> <p>Let’s go step by step.</p> <hr/> <h2 id="loss-functions-measuring-error">Loss Functions: Measuring Error</h2> <p>A neural network learns by minimizing error, and to measure this error, we use a <strong>loss function</strong>.</p> <p>A loss function compares the network’s predicted output <strong>\(y_{\text{pred}}\)</strong> with the actual output <strong>\(y_{\text{true}}\)</strong> and gives a numerical value indicating how bad the prediction was. The goal is to make this number as <strong>small as possible</strong>.</p> <h3 id="common-loss-functions">Common Loss Functions</h3> <p>Depending on the type of problem, different loss functions are used:</p> <h6 id="1-mean-squared-error-mse--for-regression-problems">1. Mean Squared Error (MSE) – For Regression Problems</h6> <p>Used when predicting continuous values. It calculates the squared difference between the actual and predicted values:</p> \[L = \frac{1}{n} \sum (y_{\text{true}} - y_{\text{pred}})^2\] <ul> <li>Large errors contribute more due to squaring.</li> <li>Encourages smaller differences between predicted and actual values.</li> </ul> <h6 id="2-cross-entropy-loss--for-classification-problems">2. Cross-Entropy Loss – For Classification Problems</h6> <p>Used when predicting categories (e.g., cat vs. dog). If we have multiple classes, cross-entropy loss is used to measure how different the predicted probability distribution is from the actual labels:</p> \[L = -\sum y_{\text{true}} \log(y_{\text{pred}})\] <ul> <li>Punishes incorrect confident predictions heavily.</li> <li>Encourages the model to assign high probabilities to the correct class.</li> </ul> <h3 id="why-is-loss-important">Why is Loss Important?</h3> <p>The loss function tells us <strong>how good or bad</strong> our neural network is. But knowing this isn’t enough—we also need a way to improve the network. That’s where <strong>backpropagation</strong> comes in.</p> <hr/> <h2 id="backpropagation-how-neural-networks-learn">Backpropagation: How Neural Networks Learn</h2> <p>Backpropagation is the heart of training a neural network. It’s the process of <strong>adjusting weights and biases</strong> to reduce the loss, making the network better over time.</p> <h5 id="the-core-idea">The Core Idea</h5> <p>Backpropagation works by moving <strong>backward</strong> through the network and tweaking the weights in the direction that reduces the loss. It uses an optimization algorithm called <strong>gradient descent</strong>, which we’ll discuss shortly.</p> <h5 id="step-by-step-explanation">Step-by-Step Explanation</h5> <h6 id="1-compute-the-loss">1. Compute the Loss</h6> <p>First, we calculate the loss using a loss function (MSE or cross-entropy).</p> <h6 id="2-calculate-gradients-derivatives">2. Calculate Gradients (Derivatives)</h6> <p>The loss depends on the weights and biases. Backpropagation computes <strong>how much each weight and bias contributed to the loss</strong> using <strong>partial derivatives</strong>.</p> <p>Mathematically, we find:</p> \[\frac{\partial L}{\partial w}\] <p>This tells us:</p> <ul> <li>If \(\frac{\partial L}{\partial w}\) is <strong>positive</strong>, decreasing \(w\) will reduce loss.</li> <li>If \(\frac{\partial L}{\partial w}\) is <strong>negative</strong>, increasing \(w\) will reduce loss.</li> </ul> <h6 id="3-update-weights-using-gradient-descent">3. Update Weights Using Gradient Descent</h6> <p>We adjust each weight using <strong>gradient descent</strong>, which updates weights in the opposite direction of the gradient:</p> \[w_{\text{new}} = w_{\text{old}} - \eta \cdot \frac{\partial L}{\partial w}\] <ul> <li>\(\eta\) is the <strong>learning rate</strong>, controlling how big the step is.</li> <li>A small learning rate makes learning slow.</li> <li>A large learning rate can cause instability.</li> </ul> <h6 id="4-repeat-until-convergence">4. Repeat Until Convergence</h6> <ul> <li>We perform this update for all weights and biases.</li> <li>The process repeats for multiple iterations (epochs) until the loss is small.</li> </ul> <h4 id="understanding-gradient-descent">Understanding Gradient Descent</h4> <p>Gradient descent is how backpropagation <strong>adjusts</strong> weights to reduce the loss. It finds the lowest point on the loss surface, like rolling a ball downhill.</p> <h5 id="types-of-gradient-descent">Types of Gradient Descent</h5> <h6 id="1-batch-gradient-descent">1. Batch Gradient Descent</h6> <ul> <li>Uses all training data to compute gradients.</li> <li>More stable but slower.</li> </ul> <h6 id="2-stochastic-gradient-descent-sgd">2. Stochastic Gradient Descent (SGD)</h6> <ul> <li>Updates weights after each training example.</li> <li>Faster but noisier updates.</li> </ul> <h6 id="3-mini-batch-gradient-descent">3. Mini-Batch Gradient Descent</h6> <ul> <li>Uses small batches of data for updates.</li> <li>Balances speed and stability.</li> </ul> <h5 id="why-does-backpropagation-work">Why Does Backpropagation Work?</h5> <p>Backpropagation is based on the <strong>chain rule of calculus</strong>. Since a neural network is just a <strong>nested function</strong>, we compute derivatives layer by layer:</p> \[\frac{\partial L}{\partial w} = \frac{\partial L}{\partial a} \times \frac{\partial a}{\partial z} \times \frac{\partial z}{\partial w}\] <p>Each layer gets its error <strong>gradually corrected</strong> by passing gradients backward, ensuring earlier layers learn the right representations.</p> <hr/> <h2 id="summary-the-learning-cycle">Summary: The Learning Cycle</h2> <ol> <li> <p>Forward Propagation: Inputs pass through the network to generate predictions.</p> </li> <li> <p>Compute Loss: Compare predictions to actual values.</p> </li> <li> <p>Backpropagation: Compute gradients of loss w.r.t. weights.</p> </li> <li> <p>Gradient Descent:Adjust weights to minimize loss.</p> </li> <li> <p>Repeat: Continue until loss is small.</p> </li> </ol> <p>This is how neural networks learn—by continuously refining their weights using loss functions and backpropagation.</p>]]></content><author><name></name></author><category term="machine-learning,"/><category term="math"/><category term="ai,"/><category term="neural-networks"/><summary type="html"><![CDATA[A description of how artificial neural networks work at code and what is the idea behind them.]]></summary></entry><entry><title type="html">Introduction to Vector Databases</title><link href="https://usyntest.github.io/blog/2024/vector-databases/" rel="alternate" type="text/html" title="Introduction to Vector Databases"/><published>2024-12-24T16:40:16+00:00</published><updated>2024-12-24T16:40:16+00:00</updated><id>https://usyntest.github.io/blog/2024/vector-databases</id><content type="html" xml:base="https://usyntest.github.io/blog/2024/vector-databases/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Traditional databases are great at storing and retrieving structured data—user records, transactions, logs—where exact matching works well. But what if you want to search for similar things instead of exact matches?</p> <p>Think of searching for an image of a golden retriever. A traditional database would need exact metadata tags like <code class="language-plaintext highlighter-rouge">"dog"</code> or <code class="language-plaintext highlighter-rouge">"golden retriever"</code> to match it. But what if you only had the image itself? This is where vector databases come in.</p> <p>Vector databases store high-dimensional embeddings, numerical representations of unstructured data like text, images, and audio. Instead of keyword-based searches, they use Approximate Nearest Neighbor (ANN) search to find similar vectors based on distance metrics like cosine similarity or Euclidean distance.</p> <p>With the rise of AI and machine learning, applications like semantic search, recommendation systems, anomaly detection, and AI chatbots rely heavily on vector search. While some traditional databases offer vector search extensions (e.g., PostgreSQL’s pgvector), dedicated vector databases like Milvus, Qdrant, Weaviate, and Pinecone are built specifically for high-speed, large-scale similarity search.</p> <p>In this blog, I’ll break down:</p> <ol> <li>What vector databases are and how they work?</li> <li>How they differ from SQL and NoSQL databases?</li> <li>Why they are essential for modern AI-driven applications?</li> <li>The key indexing techniques behind vector search</li> <li>How to get started with vector databases?</li> </ol> <hr/> <h2 id="what-are-vector-databases">What are Vector Databases?</h2> <p>Vector databases are specialized databases designed to store and retrieve high-dimensional vector embeddings efficiently. Unlike traditional databases that rely on exact matching, vector databases perform similarity search by comparing numerical representations of data points.</p> <h5 id="key-concept-storing-and-searching-embeddings">Key Concept: Storing and Searching Embeddings</h5> <p>Machine learning models can convert unstructured data—text, images, audio—into vector embeddings, which are numerical arrays that capture the meaning and relationships within the data. These embeddings are stored in a vector database, allowing for Approximate Nearest Neighbor (ANN) search to quickly find similar items.</p> <p>For example:</p> <ul> <li>A text embedding might represent the meaning of a sentence, enabling semantic search (e.g., <code class="language-plaintext highlighter-rouge">"cheap laptop"</code> matching <code class="language-plaintext highlighter-rouge">"affordable notebook"</code>).</li> <li>An image embedding might encode visual features, enabling image similarity search (e.g., retrieving dog photos similar to a given one).</li> </ul> <p>Instead of retrieving exact matches, vector databases return results ranked by how similar they are to the query, based on distance metrics like cosine similarity, Euclidean distance, or dot product.</p> <hr/> <h2 id="how-vector-search-works">How Vector Search Works</h2> <p>Vector search operates on the principle of <strong>finding the closest vectors in high-dimensional space</strong>. Since storing and searching millions (or billions) of vectors is computationally expensive, efficient algorithms like Approximate Nearest Neighbor (ANN) search are used instead of brute-force comparisons.</p> <h5 id="distance-metrics-for-similarity-search">Distance Metrics for Similarity Search</h5> <p>To determine how “close” or “similar” two vectors are, vector databases use distance functions such as:</p> <h6 id="1-euclidean-distance-l2-norm">1. Euclidean Distance (L2 norm)</h6> <p>Measures the straight-line distance between two points in space. Best for numeric data and structured patterns.</p> \[d(x, y) = \sqrt{\sum (x_i - y_i)^2}\] <h6 id="2-cosine-similarity">2. Cosine Similarity</h6> <p>Measures the angle between two vectors, focusing on direction rather than magnitude. Often used for <strong>text embeddings</strong>.</p> \[\text{similarity} = \frac{x \cdot y}{\|x\| \|y\|}\] <h6 id="3-dot-product-similarity">3. Dot Product Similarity</h6> <p>A variation of cosine similarity, often used in recommendation systems where larger values indicate stronger similarity.</p> \[\text{similarity} = x \cdot y\] <p>Depending on the use case (text search, image retrieval, recommendation systems), different databases and models optimize for specific distance metrics.</p> <hr/> <h2 id="indexing-techniques-in-vector-databases">Indexing Techniques in Vector Databases</h2> <p>Since performing a brute-force search across millions of vectors is too slow, vector databases use indexing techniques to speed up search by organizing and clustering vectors efficiently. Here are some of the most common indexing methods:</p> <h6 id="1-hnsw-hierarchical-navigable-small-world">1. HNSW (Hierarchical Navigable Small World)</h6> <ul> <li>A graph-based approach that builds a multi-layered network of vectors.</li> <li>Uses greedy search to quickly navigate through connected nodes to find the nearest neighbors.</li> <li>Provides fast and accurate searches, widely used in Qdrant, FAISS, and Milvus.</li> </ul> <h6 id="2-ivf-inverted-file-index">2. IVF (Inverted File Index)</h6> <ul> <li>Clusters vectors into Voronoi cells and searches only within the relevant cluster.</li> <li>Instead of scanning all vectors, it narrows down search space, making it memory efficient.</li> <li>Used in FAISS and Milvus for large-scale vector retrieval.</li> </ul> <h6 id="3-pq-product-quantization">3. PQ (Product Quantization)</h6> <ul> <li>Compresses high-dimensional vectors into smaller representations using quantization techniques.</li> <li>Useful when dealing with limited memory resources, as it reduces the storage size of vectors.</li> <li>Works well in combination with IVF for scalable vector search.</li> </ul> <h5 id="choosing-the-right-indexing-method">Choosing the Right Indexing Method</h5> <table> <thead> <tr> <th>Method</th> <th>Best For</th> <th>Pros</th> <th>Cons</th> </tr> </thead> <tbody> <tr> <td>HNSW</td> <td>High-accuracy search</td> <td>Fast &amp; scalable</td> <td>High memory usage</td> </tr> <tr> <td>IVF</td> <td>Large-scale retrieval</td> <td>Memory-efficient</td> <td>Lower recall for small datasets</td> </tr> <tr> <td>PQ</td> <td>Memory optimization</td> <td>Reduces storage size</td> <td>Slight drop in accuracy</td> </tr> </tbody> </table> <blockquote> <p><strong>Note:</strong> Each indexing method balances speed, memory efficiency, and accuracy. The right choice depends on the dataset size, query speed requirements, and available resources.</p> </blockquote> <hr/> <h2 id="use-cases-of-vector-databases">Use Cases of Vector Databases</h2> <p>Vector databases power many AI-driven applications where <strong>similarity search</strong> is crucial. Instead of relying on exact keyword matches, these databases allow systems to find <strong>semantically similar</strong> results, making them essential for:</p> <h6 id="1-semantic-search">1. Semantic Search</h6> <p>Traditional keyword-based search systems struggle with synonyms and contextual meaning. Vector databases enable <strong>semantic search</strong>, where search queries are matched based on their meaning rather than exact words.</p> <p><strong>Example:</strong> Searching for <code class="language-plaintext highlighter-rouge">"affordable phone"</code> returns <code class="language-plaintext highlighter-rouge">"cheap smartphone deals"</code> even though the words are different.</p> <h6 id="2-image--video-search">2. Image &amp; Video Search</h6> <p>Vector embeddings allow <strong>reverse image search</strong>, where users can upload an image to find visually similar ones.</p> <p><strong>Example:</strong> Google Lens and Pinterest use vector embeddings to match images with similar content.</p> <h6 id="3-personalized-recommendations">3. Personalized Recommendations</h6> <p>Recommendation systems use vector embeddings to suggest <strong>similar items</strong> based on user preferences.</p> <p><strong>Example:</strong></p> <ul> <li><strong>E-commerce:</strong> <code class="language-plaintext highlighter-rouge">"Users who liked this product also liked..."</code></li> <li><strong>Streaming services:</strong> <code class="language-plaintext highlighter-rouge">"Because you watched X, you might like Y"</code></li> </ul> <h6 id="4-anomaly-detection--fraud-prevention">4. Anomaly Detection &amp; Fraud Prevention</h6> <p>Vector search helps detect anomalies by identifying outliers in high-dimensional data.</p> <p><strong>Example:</strong></p> <ul> <li><strong>Finance:</strong> Detecting fraudulent transactions by spotting unusual spending patterns.</li> <li><strong>Cybersecurity:</strong> Identifying unusual login behavior in a system.</li> </ul> <h6 id="5-ai-chatbots--rag-retrieval-augmented-generation">5. AI Chatbots &amp; RAG (Retrieval-Augmented Generation)</h6> <p>Vector databases improve AI chatbots by enabling <strong>context-aware responses</strong> through similarity search.</p> <p><strong>Example:</strong> ChatGPT-like assistants use vector search to retrieve relevant documents for better answers.</p> <h2 id="challenges--limitations">Challenges &amp; Limitations</h2> <p>While vector databases are powerful, they come with trade-offs:</p> <h6 id="1-memory-usage">1. Memory Usage</h6> <ul> <li>Graph-based indexing (HNSW) can be <strong>memory-intensive</strong>, requiring large RAM for high recall.</li> <li>PQ and IVF reduce memory usage but may sacrifice accuracy.</li> </ul> <h6 id="2-indexing-speed-vs-query-speed">2. Indexing Speed vs. Query Speed</h6> <ul> <li><strong>HNSW:</strong> Fast queries but slow indexing.</li> <li><strong>IVF:</strong> Faster indexing but slightly slower queries.</li> <li><strong>PQ:</strong> Best for reducing memory but adds latency to queries.</li> </ul> <h6 id="3-precision-vs-recall">3. Precision vs. Recall</h6> <ul> <li>Approximate nearest neighbor (ANN) methods <strong>prioritize speed over perfect accuracy</strong>.</li> <li>Fine-tuning parameters like <strong>ef_construction (HNSW) or nprobe (IVF)</strong> is required to balance speed and accuracy.</li> </ul> <hr/> <h2 id="getting-started-with-a-vector-database-qdrant-example">Getting Started with a Vector Database (Qdrant Example)</h2> <p>Let’s set up <strong>Qdrant</strong>, an open-source vector database, and run a <strong>basic similarity search</strong>.</p> <h6 id="1-install-qdrant-server">1. Install Qdrant Server</h6> <p>Run Qdrant locally using Docker:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker run <span class="nt">-p</span> 6333:6333 qdrant/qdrant
</code></pre></div></div> <h6 id="2-install-qdrant-client">2. Install Qdrant Client</h6> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>qdrant-client numpy
</code></pre></div></div> <h6 id="3-create-a-collection--insert-vectors">3. Create a Collection &amp; Insert Vectors</h6> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">qdrant_client</span> <span class="kn">import</span> <span class="n">QdrantClient</span>
<span class="kn">from</span> <span class="n">qdrant_client.models</span> <span class="kn">import</span> <span class="n">PointStruct</span>

<span class="c1"># Connect to Qdrant server
</span><span class="n">client</span> <span class="o">=</span> <span class="nc">QdrantClient</span><span class="p">(</span><span class="sh">"</span><span class="s">localhost</span><span class="sh">"</span><span class="p">,</span> <span class="n">port</span><span class="o">=</span><span class="mi">6333</span><span class="p">)</span>

<span class="c1"># Create a collection
</span><span class="n">client</span><span class="p">.</span><span class="nf">recreate_collection</span><span class="p">(</span><span class="n">collection_name</span><span class="o">=</span><span class="sh">"</span><span class="s">my_vectors</span><span class="sh">"</span><span class="p">,</span> <span class="n">vector_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">distance</span><span class="o">=</span><span class="sh">"</span><span class="s">Cosine</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Insert some vectors
</span><span class="n">vectors</span> <span class="o">=</span> <span class="p">[</span>
    <span class="nc">PointStruct</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">vector</span><span class="o">=</span><span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]),</span>
    <span class="nc">PointStruct</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">vector</span><span class="o">=</span><span class="p">[</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">]),</span>
    <span class="nc">PointStruct</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">vector</span><span class="o">=</span><span class="p">[</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">]),</span>
<span class="p">]</span>
<span class="n">client</span><span class="p">.</span><span class="nf">upsert</span><span class="p">(</span><span class="n">collection_name</span><span class="o">=</span><span class="sh">"</span><span class="s">my_vectors</span><span class="sh">"</span><span class="p">,</span> <span class="n">points</span><span class="o">=</span><span class="n">vectors</span><span class="p">)</span>
</code></pre></div></div> <h6 id="4-perform-a-similarity-search">4. Perform a Similarity Search</h6> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">query_vector</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.15</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.35</span><span class="p">]</span>

<span class="n">results</span> <span class="o">=</span> <span class="n">client</span><span class="p">.</span><span class="nf">search</span><span class="p">(</span>
    <span class="n">collection_name</span><span class="o">=</span><span class="sh">"</span><span class="s">my_vectors</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">query_vector</span><span class="o">=</span><span class="n">query_vector</span><span class="p">,</span>
    <span class="n">limit</span><span class="o">=</span><span class="mi">2</span>  <span class="c1"># Return top 2 similar vectors
</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>
</code></pre></div></div> <p>This will return the <strong>most similar vectors</strong> based on cosine similarity! 🎯</p> <hr/> <h2 id="conclusion">Conclusion</h2> <p>Vector databases are transforming AI-driven applications by enabling fast, scalable similarity search for text, images, audio, and more. As models get larger and embeddings become a core part of AI workflows, vector search will be a crucial component of modern data infrastructure.</p> <h5 id="future-trends-">Future Trends 🚀</h5> <p>✅ Hybrid Search – Combining vector and keyword search for better retrieval.<br/> ✅ Efficient Indexing – Reducing memory usage with better compression.<br/> ✅ Cloud-native Vector Databases – Fully managed solutions like Pinecone.<br/> ✅ RAG-powered AI Agents – Better AI models using retrieval-augmented generation.</p> <p>With the rise of LLMs and AI-powered search, vector databases will only become more important in the future. If you haven’t explored them yet, now’s the perfect time to start! 🚀</p>]]></content><author><name></name></author><category term="database"/><category term="search,"/><category term="vector-database,"/><category term="ai"/><summary type="html"><![CDATA[Modern applications require more than just exact search—they need similarity search, and vector databases enable this at scale.]]></summary></entry></feed>